# Introduction

The existence, and nature, of a distinction between short and long term memory has been debated for over a century \cite{James1890,Jonides2008}. Associative buffer models, such as Raaijmakers and Shiffrin’s Search of Associative Memory \cite[SAM, ]{Raaijmakers1981}, present computational implementations of the highly influential theory of separate but interacting short and long-term memory processes \cite{Atkinson1968,Baddeley1974}.

The features of SAM illustrate the canonical dual-storage algorithm: Items to be recalled are entered into a temporary storage buffer, replacing prior items if the buffer is full, and therein form episodic associations with other buffer items, and perhaps also a context marker. Those associations then serve as cues for consecutive recalls, producing many of the familiar dynamics of list recall performance \cite{Davelaar2005}.

The Linking via Active Maintenance Model (LAMM) aims to explore the biological feasibility of associative buffer memory theories by implementing them as neuronal networks. Additionally, it advances the debate between dual- and single-memory models \cite{Brown2002,Howard2002,Cowan2008} by arguing that, when implemented neuronally, associative buffer networks incorporate temporal context dynamics \cite{Usher2008}. Finally, LAMM’s architecture is based upon prior work showing that buffer mediated associations are necessary to explain the effects of degraded stimuli on recall \cite{Piquado2010,Miller2010,Cousins2014}.

## Word list recall

- Is an experimental paradigm with long history as a model system for short term memory and rich behavioural phenomena.

Recall for sequentially presented lists of items has long been a key experimental paradigm for revealing hidden structure in memory dynamics. Dependencies on list-position, presentation timing, temporal contiguity, effects of rehearsal, intra- and inter-list interference, categorical and acoustic relatedness, and scaling laws have all been observed \cite{Murdock1962,Rundus1971,Kahana1996,Golomb2008,Grenfell-Essam2012,Cowan2008a,Farrell2011a} (more!). Disruptions and distractions have been used to probe the stages and modalities of memory encoding \cite{Carroll2010,Elliott1998,Spataro2013,Cowan2008} including the finding of a retroactive effect indicating that encoding into short term memory continues beyond the removal of the stimulus \cite{Rabbitt1968,Cousins2014}.

The canonical result in free-recall experiments is the 'U'-shape performance curve for recall of items as a function of presentation order: the so-called serial position effects. Recall is stronger for both early items (denoted 'primacy'), and late items ('recency'), than for middle list items. Further, shorter lists seem to favour primacy over recency, and vice versa. Distractor tasks just before recall reduce recency but nor primacy, whereas distraction throughout both presentation and recall restore somewhat the concavity of the serial-position curve \cite{Howard1999}. On the other hand, subjects tend to overtly rehearse early list items much more than late ones; and whilst recall increases with rehearsal, late items are recalled much more easily than early ones for a given proportion of allocated rehearsal time \cite{Rundus1971}. It thus appears that different mechanisms may support the recall of early vs late items \cite[but see discussions in ]{Sederberg2008,Usher2008}.

- several models proposed, notably differential memory for late list items key support for the idea of the short term memory buffer.

A key point of disagreement within the literature -- and entwined in the early- vs late-items dissociation mentioned above -- is whether memory for recently perceived list items is computed by the same processes that encode and retrieve general long term memory (LTM) (the 'unitary view'), or by dedicated, temporally limited 'short-term' processes (the 'dual-store' or 'dual-process' view) \cite{Raaijmakers1981a,Davelaar2005,Sederberg2008}. Generally, unitary models have relied on representations of distinctiveness between items to be remembered, which contain some invariance to timsecales \cite{Brown2002,Howard2002}, whilst dual-process models have posited a separate, short-term, actively maintained or reverberatory storage, or 'buffer' \cite{Baddeley1974,Raaijmakers1981,Davelaar2005} (more!). More recently, structural similarities have been pointed out between actual computational implementations of the different theories \cite{Usher2008}, and modeling has suggested that both distinctiveness and buffer mechanisms may be necessary \cite{Piquado2010,Miller2010,Cousins2014}. In most cases such short-term processes are assumed to contribute most strongly to memory for only very recently perceived stimuli, such as the ends of a word list, so that theoretical disagreement focuses on how these 'recent' items are remembered. On the other hand, few models provide an integrated account of the primacy effect, and even fewer still a biological one \cite{Lansner2013,Farrell2012a}, and our model is guilty of this shortcoming. We will argue that primacy should be a focus of future work in this area. 


## Memory theories, summary of trends

- Buffers (active maintenance) vs similarity (interference)

The debate regarding the mechanisms operating in list recall tasks is one front in the larger debate about short-term memory in general: that between buffer maintenance and attention-driven theories. Buffer theories posit that the most recently perceived stimuli are maintained in short term memory buffers via active mechanisms \cite{Baddeley1974,Baddeley2010,Davelaar2005,Grossberg1978,Bradski1994}. This view has won empirical support, especially from electrophysiological recordings from behaving primates \cite{Funahashi1989,Fuster1971,Fuster1973,Miyashita1988,Miyashita1988a}, coalescing into a 'standard model' of short term memory \cite{Goldman-Rakic1987,Goldman-Rakic1990,Courtney2004,Postle2006}.

Attention driven theories propose that short-term memory is simply the process of temporarily re-activating, or 'attending' to, memory representations stored in LTM \cite{Cowan1993,Cowan2008} (more?). Such theories reject Baddeley's \cite{Baddeley1974} physical separation of short- from long-term processing, and instead propose a functional separation only, consistent with Atkinson & Shiffrin's \cite{Atkinson1968} original proposal, and, in fact, the Hebbian idea of 'transient memory' \cite{Hebb1949}. This approach avoids the undesirable requirement of duplicating representations of all stored knowledge in dedicated buffer networks organised by information type and stimulus modality, as implied in Baddeley's compartmental memory model \cite{Baddeley2010}. Multiple LTM memories can instead be reactivated at the same time within their existing LTM media, requiring no duplicate networks. Only a subset of such reactivated memories are considered in the 'focus' of attention as generally understood, allowing for gradations in the level of focus and active maintenance \cite{Cowan2008}: In the case of word list recall, the representation of the current word would be reactivated most strongly and be 'in focus', with less recent words fading to baseline.

In this way \cite{Cowan2008} leaves open the possibility for both decay of active maintenance and interference between stored representations as mechanisms of forgetting, which is a further point of conflict between buffer and attentional theories \cite{Jonides2008}. In buffer theories recall fails when buffer activity decays. Attentional theories, by contrast, traditionally operate on memory encoding schemes in which memories which are most different, either in content, context, or time-of-perception, suffer least mutual interference from other encoded memories, and hence have greater recall \cite{Hopfield1982,Nairne1997,Brown2002,Brown2007,Howard2002}. The Temporal Context Model (TCM) of \cite{Howard2002,Sederberg2008} binds memories to an evolving 'current context' representation, which, by its nature, also includes the content of the memory itself. TCM is one of the most successful short term memory models, but in constructing their context representation from superpositions of stimuli it becomes, in implementation, approximately an activation-based buffer mechanism with decay \cite{Usher2008}.

Given, firstly, the natural representational overlap between context and its constituent parts, as evident in TCM; secondly, evidence suggesting that both buffer- and context-like mechanisms may be needed to explain observed recall phenomena \cite{Miller2010,Cousins2014}; and the efficiency advantage in constructing short-term memory as a process operating on LTM representations \cite{Raaijmakers1981a,Davelaar2013,Postle2006} rather than require a potentially combinatoric duplication of LTM; we propose a novel short-term memory network with the following properties: The LAMM memory network is an activation based buffer, which represents stimuli agnostically to the stimulus type or modality, and in which items being remembered are also associated with one another, encoding contextual information about such multi-item episodes.

## Linking Via Active Maintenance

- outline LAMM here

The Linking via Active Maintenance model is premised upon the two known mechanisms of neuronal memory: active maintenance and synaptic plasticity, both of which were also present in SAM \cite{Raaijmakers1981}. In this way it aims to combine aspects of both the active buffer and contextual association models that have proven successful. Moreover, these processes are not independent, but interact according to the standard Hebbian paradigm \cite{Hebb1949,Amit1995} wherein the repeated firing of maintained activity leads to strengthening of the connections which support it, thus 'wiring in' this activity pattern for later reactivation.

LAMM is structured into two network layers: a long term memory (LTM) store in which all the presented items are already encoded, and the short-term network through which episodic information about the lists presented is maintained and encoded. The item layer, as we shall call the LTM network, forms both the input and output of the model. Its structure is a winner-take-all attractor network, consisting of rate-model units approximating recurrently connected neuronal subpopulations, only one of which may be active at a time. Each such state, which are orthogonal, represents the perception of a single word item, implying the definite end state of linguistic discrimination processes which recognise spoken language (CITE?). We assume the this end state activity constitutes conscious perception of the word, and thus the reactivation of such states in the absence of auditory stimuli constitutes recollection. In this way the LTM item layer is both input and output for short term memory layer, which we will refer to simply as the memory network.

The memory network also contains self-recurrent units but its dynamics admits several such units to be active simultaneously, allowing multiple 'winners'. Memory units share significant lateral excitation, and can be bound together into larger subgroups via associative plasticity. Activity in the memory network is excited by feedforward connections from the item layer, which provides feedback to items as well (see Table ?). These item-memory, memory-item and memory-memory connections undergo bi-directional, short-term potentiation, and largely encode the episodic associations of the presented lists. Their plasticity dynamics are based upon recently demonstrated, associative short term plasticity \cite[ASTP, ]{Erickson2010}, which quickly potentiates synapses of co-active neurons, and then decays over several minutes.

LAMM's memory layer also aims to meet three criteria: Firstly, that it be limited in capacity, and hence must be modality- and type-agnostic, unlike the potentially combinatoric components of a fully compartmental Baddley-style memory \cite{Courtney2004,Postle2006}; secondly, it have an intermediate dimensionality, small enough such that memory activity is discrete between representations of different list items, but large enough that these overlap and may also represent the current context via such superposition of recent items \cite{Usher2008}; finally, that the activity evoked in the memory layer be variable such that it depends on recent history, and hence that repeated stimuli may evoke different patterns of activity each time it is presented to the network. The second property is a statement of distributed coding, that memory activity representing different items may overlap; the third property requires that these patterns of activity not be determined solely by the input connectivity. We shall see that the parameters that were found to generate recall most reliably took the memory dynamics away from the third property, with the result that memory activity is largely determined by item-memory projections, and thus that memory representations are largely fixed (Fig. \ref{memcorrs}).


## Comparison to other models - Most similar models

LAMM's mechanisms are most similar to Raaijmaker and Shiffrin's Search of Associative Memory (SAM) \cite{Raaijmakers1981}, and in some ways can be considered a biological implementation of those ideas. LAMM shares much with \cite{Davelaar2005} in general principles, but their and our memory networks are differently designed: their memory activity is localist and evolves stochastically via a random walk, independently of items presented; in LAMM memory activity is distributed amongst memory units, and is driven by feedforward inputs from the item layer. LAMM's memory network could instead be seen as a crude neuronal implementation of TCM's context signal, in that memory activity both reflects and is driven by presented items, yet we do not have the perfect fidelity of representation that TCM's infinite dimensional context vector allows. In these ways LAMM's contextual representation is more discrete and variable than TCM's but less so than Davelaar et al.'s.

- The LTM network comprising the other half of LAMM operates in a similar fashion to the memory model of Verduzco-Flores.

\citet{Verduzco-Flores2012} use a single LTM-like layer, with a 2nd layer extension, also consisting of a winner-take-all rate-model network. In their model, the number of winners is varied in order to achieve cumulative inputs \cite[cf ]{Jensen1996b} to maintain and disambiguate sequences. This connectivity functions as a momentum in the dynamics. Having k co-active units allows their model to encode sequences which have repeated terms k positions apart (e.g. ABAC, with k=2). In a second mode of operation, Verduzco-Flores et al. set up long repeating cycles in their first layer to act as timing signals, to which arbitrary sequences in a second layer can then be coupled. This mode is rather like \cite{Davelaar2005} except biasing the random-walk context to move only in one direction. We may note that varying a bias term in this way would have the effect of shifting the behaviour between serial recall performance and the free recall LAMM aims to reproduce -- the dual-layer operation of \cite{Verduzco-Flores2012} aims at serial recall, but when subjects are allowed to recall list items in any order they wish transitions between non-sequential items and reverse-order recall are common, making reliably serial mechanisms, in our view, unlikely foundations for episodic memory in general.

\citet{Lansner2013} propose a free-recall memory network very similar to the item layer we use in LAMM, with the major difference being that their units are arranged in a columnar structure, such that units within each winner-take-all column compete for activation, but are co-active with other winners from other columns. Lansner et al. show that reasonable behavior can be obtained within a single (LTM) network. This seems to depend on multiple reactivations during the presentation phase, especially for early list items, a reactivation pattern that is consistent with experiments involving overt rehearsal \cite{Rundus1971}. However, when such reactivations are suppressed (to model experiments with distractors aimed at eliminating it), Lansner et al's model predicts very different behaviour to that observed, perhaps indicating that rehearsal is too central to their operation. LAMM also exhibits such rehearsal-like reactivations, and early-item recall increases strongly with reactivation, but does not require it as appears to be the case in \cite{Lansner2013} (Fig S?, c.f. Lansner et al. Fig 2D).


- Process models, recall probability laws, biological mechanistic models * Recall laws as defining a probability for recalls given parameters

Most generally, list-memory models may be classified into two types: process models, which include mechanistic models of both list encoding and recall, and simulate the experimental paradigm being considered \cite{Raaijmakers1981,Davelaar2005,Sederberg2008,Piquado2010,Farrell2012a,Verduzco-Flores2012,Lehman2013,Lansner2013,Cousins2014,Takac2015}; and encoding algorithms, which define a representation for memory encoding, but do not explicitly simulate the process of encoding and recall under any particular experiment \cite{Murdock1997,Howard2002,Brown2002,Brown2007,Afraimovich2004}. Such models span a wide range from simple network models such as \cite{Davelaar2005,Verduzco-Flores2012,Lansner2013} and ours, or more elaborate and multiple processes \cite{Lehman2013,Farrell2012a}, and also multiple networks as well \cite{Bradski1994,Takac2015}. A smaller number of process models have defined detailed biological mechanisms \cite{Lisman1995,Mongillo2008,Szatmary2010}, but these have largely focussed on memory for serial order, and not considered free-recall dynamics.